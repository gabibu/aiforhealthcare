{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AgBH-n-LuIVH"
      },
      "outputs": [],
      "source": [
        "#baseline model UNET\n",
        "# I splitted every image to patches of 64 * 64 pixels \n",
        "#and classified every pixel in every patch as binary classification  task\n",
        "\n",
        "\n",
        "#so for the model the input is 64 * 64 pixels images.\n",
        "\n",
        "\n",
        "#model: UNET\n",
        "#loss Binary cross entropy \n",
        "#optimizer: SGD\n",
        "\n",
        "#I splitted to dataset under to dicom-images-train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_WGEQ2veoM",
        "outputId": "1a7f3ca2-8cf1-4a62-dd05-7303abb1bf82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d0dXiKcpuIVI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from skimage import io, transform\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from imageio import imread\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import cm\n",
        "\n",
        "import pydicom\n",
        "from abc import ABC, abstractmethod\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, RandomRotation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask2rle(img, width, height):\n",
        "    rle = []\n",
        "    lastColor = 0;\n",
        "    currentPixel = 0;\n",
        "    runStart = -1;\n",
        "    runLength = 0;\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            currentColor = img[x][y]\n",
        "            if currentColor != lastColor:\n",
        "                if currentColor == 255:\n",
        "                    runStart = currentPixel;\n",
        "                    runLength = 1;\n",
        "                else:\n",
        "                    rle.append(str(runStart));\n",
        "                    rle.append(str(runLength));\n",
        "                    runStart = -1;\n",
        "                    runLength = 0;\n",
        "                    currentPixel = 0;\n",
        "            elif runStart > -1:\n",
        "                runLength += 1\n",
        "            lastColor = currentColor;\n",
        "            currentPixel+=1;\n",
        "\n",
        "    return \" \".join(rle)\n",
        "\n",
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle.split()])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    for index, start in enumerate(starts):\n",
        "        current_position += start\n",
        "        mask[current_position:current_position+lengths[index]] = 255\n",
        "        current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height)\n"
      ],
      "metadata": {
        "id": "9WobemP_uaCJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "7I1nfKTOuIVJ",
        "outputId": "3bb56339-826e-4577-acd6-92b9e123fcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__pyTorch VERSION: 1.11.0+cu113\n",
            "use_cuda = True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use_cuda = {0}'.format(use_cuda))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DATA_FOLDER ='data'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSClFvEdufgZ",
        "outputId": "505384e8-8317-4cf1-8ba2-42839bf5689d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/medical\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TegtM3JzukYY",
        "outputId": "db9a8089-4116-4d3c-eefa-033d1360d119"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dicom-images-train.zip\ttrain-rle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/medical/dicom-images-train.zip\" \"dicom-images-train.zip\"\n"
      ],
      "metadata": {
        "id": "HqnhMTHTumyP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/medical/train-rle.csv' \"train-rle.csv\""
      ],
      "metadata": {
        "id": "kS2rUiZkup8Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zhOcM3Gusxj",
        "outputId": "ef660d46-4ed3-4fdf-90cc-6c873c24f3ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  dicom-images-train.zip  drive  sample_data  train-rle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dicom-images-train.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrcW5B7Yu6U-",
        "outputId": "06eea54a-11e3-494c-91b2-34296e4e9981"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dicom-images-train.zip\n",
            "replace data/dicom-images-train/1.2.276.0.7230010.3.1.2.8323329.4366.1517875182.502231/1.2.276.0.7230010.3.1.3.8323329.4366.1517875182.502230/1.2.276.0.7230010.3.1.4.8323329.4366.1517875182.502232.dcm? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Sn3O5DuIVK",
        "outputId": "dde50cdc-6c27-4edb-95f2-eee4ce3ef43e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "SEED = 999\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "fixSeed(SEED)\n",
        "SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "6RpGMsUeuIVK",
        "outputId": "4b5ef023-703a-456f-84aa-21c560463957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                        EncodedPixels\n",
              "ImageId                                                                                              \n",
              "1.2.276.0.7230010.3.1.4.8323329.14099.151787524...   745018 1 1024 3 1021 5 1019 8 1016 10 1015 12..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7aa9673f-f42d-4ee9-8427-436ad4636a54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EncodedPixels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageId</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.2.276.0.7230010.3.1.4.8323329.14099.1517875249.625073</th>\n",
              "      <td>745018 1 1024 3 1021 5 1019 8 1016 10 1015 12...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7aa9673f-f42d-4ee9-8427-436ad4636a54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7aa9673f-f42d-4ee9-8427-436ad4636a54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7aa9673f-f42d-4ee9-8427-436ad4636a54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# df = pd.read_csv('archive/train-rle.csv', header=None, index_col=0)\n",
        "# train_fns = sorted(glob.glob('archive/dicom-images-train/*/*/*.dcm'))#[0:500]\n",
        "# df_full = pd.read_csv('archive/train-rle.csv', index_col='ImageId')\n",
        "\n",
        "df = pd.read_csv('train-rle.csv', header=None, index_col=0)\n",
        "train_fns = sorted(glob.glob('data/dicom-images-train/*/*/*.dcm'))\n",
        "len(train_fns)\n",
        "df_full = pd.read_csv('train-rle.csv', index_col='ImageId')\n",
        "df_full.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_full[df_full[' EncodedPixels'].str.contains('-1')].shape, df_full[~df_full[' EncodedPixels'].str.contains('-1')].shape, df_full.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpll1gpmz7d9",
        "outputId": "9b3197f8-3590-4876-8a29-c388aa2b7ed0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8296, 1), (3286, 1), (11582, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_full.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSqKipKZ0tds",
        "outputId": "689b27ad-ed9d-453c-bbaa-673999e92e07"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' EncodedPixels'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = df_full.reset_index().drop_duplicates(['ImageId'])\n",
        "\n",
        "\n",
        "negative_images =  set(dataset_df[dataset_df[' EncodedPixels'].str.contains('-1')].sample(250)['ImageId'].tolist())\n",
        "\n",
        "positive_images = set(dataset_df[~dataset_df[' EncodedPixels'].str.contains('-1')].sample(250)['ImageId'].tolist())\n",
        "\n",
        "images_ids_for_trainning = set.union(negative_images, positive_images)\n",
        "len(images_ids_for_trainning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgB1KX0A0Qep",
        "outputId": "e6b5817c-b0ad-4a10-cced-67695be26edf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2X5ULDt417jZ",
        "outputId": "e6dc8293-76e5-4acc-df99-950f913c95f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.276.0.7230010.3.1.4.8323329.5796.1517875190.763845'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCbOf8YuIVK",
        "outputId": "5766f680-ee5a-486d-81e9-90561aff3612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10712/10712 [00:09<00:00, 1183.32it/s]\n"
          ]
        }
      ],
      "source": [
        "im_height = 1024\n",
        "im_width = 1024\n",
        "im_chan = 1\n",
        "valid_indexes = []\n",
        "X_s = np.zeros((len(images_ids_for_trainning), im_height, im_width, im_chan), dtype=np.uint8)\n",
        "Y_s = np.zeros((len(images_ids_for_trainning), im_height, im_width, 1), dtype=bool)\n",
        "sys.stdout.flush()\n",
        "index_to_set = 0\n",
        "\n",
        "for _id in tqdm(train_fns, total=len(train_fns)):\n",
        "\n",
        "    \n",
        "    image_id = _id.split('/')[-1].split('.dcm')[0]\n",
        "\n",
        "    \n",
        "    if image_id not in images_ids_for_trainning:\n",
        "      continue\n",
        "\n",
        "\n",
        "    dataset = pydicom.read_file(_id)\n",
        "    X_s[index_to_set] = np.expand_dims(dataset.pixel_array, axis=2)\n",
        "\n",
        "  \n",
        "\n",
        "   #df_full[df_full.index == '1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027']\n",
        "\n",
        "    try:\n",
        "        if '-1' in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "            Y_s[index_to_set] = np.zeros((1024, 1024, 1))\n",
        "        else:\n",
        "            if type(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']) == str:\n",
        "                Y_s[index_to_set] = np.expand_dims(rle2mask(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels'], 1024, 1024), axis=2)\n",
        "            else:\n",
        "                Y_s[index_to_set] = np.zeros((1024, 1024, 1))\n",
        "                for x in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "                    Y_s[index_to_set] =  Y_s[index_to_set] + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n",
        "    except KeyError:\n",
        "        print(f\"Key {_id.split('/')[-1][:-4]} without mask, assuming healthy patient.\")\n",
        "        Y_s[index_to_set] = np.zeros((1024, 1024, 1)) # Assume missing masks are empty masks.\n",
        "    \n",
        "    index_to_set += 1\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_s.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez2_Jmex3Ls7",
        "outputId": "70f1e7d6-5193-4b8f-feee-4762ffe74117"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 1024, 1024, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UtCqL5ruIVL",
        "outputId": "c7254158-4d70-42d7-f907-17a143408a63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 1024, 1024, 1), (500, 1024, 1024, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X_s.shape, Y_s.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7_p-uiouIVL",
        "outputId": "2da53ad8-3908-4b5a-8f9f-4664770430ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((450, 1024, 1024, 1), (450, 1024, 1024, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "validation_size = int(Y_s.shape[0] * 0.1)\n",
        "validation_indexes = np.random.choice(range(0, Y_s.shape[0]), size=validation_size, replace=False)\n",
        "train_indexes  = [i for i in range(0, Y_s.shape[0]) if i not in validation_indexes]\n",
        "\n",
        "\n",
        "y_train = Y_s[train_indexes, :, :]\n",
        "y_validation = Y_s[validation_indexes, :, :]\n",
        "\n",
        "\n",
        "\n",
        "X_validation = X_s[validation_indexes, :, :]\n",
        "X_train = X_s[train_indexes, :, :]\n",
        "\n",
        "y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe1VQZjDuIVM",
        "outputId": "5af6fcd5-250f-4232-c305-cfbbc0f9d8c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((115200, 64, 64, 1), (115200, 64, 64, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "im_height = 64\n",
        "im_width = 64\n",
        "X_train = X_train.reshape((-1, im_height, im_width, 1))\n",
        "y_train = y_train.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "\n",
        "X_validation = X_validation.reshape((-1, im_height, im_width, 1))\n",
        "y_validation = y_validation.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l87g-WRAuIVM",
        "outputId": "8d8d22ec-bd6c-4663-8058-657874c28d5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3078488, 468780712)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "num_of_positive_pixels = np.sum(y_train)\n",
        "num_of_negatuve_pixels = y_train.size - num_of_positive_pixels \n",
        "\n",
        "num_of_positive_pixels, num_of_negatuve_pixels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0p_UIopmuIVN"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, images, masks, preprocesing = None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.preprocesing = preprocesing\n",
        "        \n",
        "      \n",
        "    def __len__(self):\n",
        "        return self.masks.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        y = self.masks[index]\n",
        "        y = np.where(y == True, 1, 0)\n",
        "        \n",
        "        image  = X_train[index]\n",
        "        \n",
        "        if self.preprocesing is not None:\n",
        "            for pre_processor in self.preprocesing:\n",
        "                \n",
        "                image = pre_processor.preprocess(image)\n",
        "                \n",
        "        \n",
        "        image = image.reshape(1, image.shape[0], image.shape[1])\n",
        "        y = y.reshape(1, y.shape[0], y.shape[1])\n",
        "        \n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkDNdPTkuIVN",
        "outputId": "ccd42caa-2d6f-450b-b1c9-ad2bce331c65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((115200, 64, 64, 1), (115200, 64, 64, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bUCGJAk2uIVO"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ekzsQ4BmuIVO"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, confusion_matrix, optimizer, criterion, input_transformation=None):\n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "\n",
        "        calc_count += labels.shape[0]\n",
        "        images = images.float().to(device)\n",
        "     \n",
        "        labels = labels.float().to(device)\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        calc_loss += loss.item()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    \n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            calc_count += labels.shape[0]\n",
        "            images = images.float().to(device)\n",
        "\n",
        "            labels = labels.float().to(device)\n",
        "            images = Variable(images)\n",
        "            labels = Variable(labels)\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            calc_loss += loss.item()\n",
        "            \n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "5oIlGdpEuIVP"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, criterion, optimizer, numer_of_epoch, train_data_loader, validation_data_loader):\n",
        "    \n",
        "    for epoch in range(10):\n",
        "        print(epoch)\n",
        "        train_loss = train(model, train_data_loader, None, optimizer, criterion, None)\n",
        "        val_loss = eval_model(model, validation_data_loader, criterion)\n",
        "\n",
        "        print('epoch {} train loss {} val loss {}'.format(epoch, train_loss, val_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "IUKbFys9uIVP"
      },
      "outputs": [],
      "source": [
        "#baseline\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss().to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "#run_experiment(model, criterion, optimizer, 100, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xh3YdS0TuIVP"
      },
      "outputs": [],
      "source": [
        "#weighted loss \n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment(model, criterion, optimizer, num_epochs, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK7yaj9MuIVQ"
      },
      "outputs": [],
      "source": [
        "class PreProcessingStep(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def preprocess(image):\n",
        "        pass\n",
        "\n",
        "    \n",
        "class PreProcessingWindow(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, preprocess_window, output_range):\n",
        "        self._preprocess_window = preprocess_window \n",
        "        self._output_range = output_range\n",
        "    \n",
        "    def preprocess(self, image):\n",
        "        windowed_image = np.interp(image, self._preprocess_window, self._output_range)\n",
        "        return windowed_image\n",
        "\n",
        "\n",
        "class PreProcessingAugmentations(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, augmentations):\n",
        "        self.augmentations = augmentations \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "      image =Image.fromarray(image.squeeze(2))\n",
        "\n",
        "      for aug in self.augmentations:\n",
        "        image = aug(image)\n",
        "\n",
        "      return np.expand_dims(np.asarray(image), 2) \n",
        "\n",
        "class PreProcessingNormalization(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "      image =Image.fromarray(image.squeeze(2))\n",
        "\n",
        "      for aug in self.augmentations:\n",
        "        image = aug(image)\n",
        "\n",
        "      return np.expand_dims(np.asarray(image), 2) \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "#mean, std = pixels.mean(), pixels.std()\n",
        "\n",
        "X_train.mean()\n",
        "mean, std = X_train.mean(), X_train.std()\n",
        "\n",
        "pixels = (X_train - mean) / std\n",
        "pixels.shape, X_train.shape\n",
        "\n",
        "yy = (X_train[0]  - mean) / std\n",
        "\n",
        "np.array_equal(yy, pixels[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls-NOpWRuIVQ"
      },
      "outputs": [],
      "source": [
        "#weighted loss with image preprocessing\n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "preprocesing = [PreProcessingWindow((-300, 700), (0., 1.))]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment(model, criterion, optimizer, 100, train_data_loader, validation_data_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted loss with image preprocessing and augmentations\n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "number_of_epocs = 2\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "aug_preprocessing = PreProcessingAugmentations([RandomHorizontalFlip(0.5), RandomVerticalFlip(0.5), RandomRotation(0.3)])\n",
        "\n",
        "preprocesing = [PreProcessingWindow((-300, 700), (0., 1.)), aug_preprocessing]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment(model, criterion, optimizer, number_of_epocs, train_data_loader, validation_data_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "YwE3VBgGEdOR",
        "outputId": "5ccbbfe5-dffb-423a-9d72-9c2bbf74c73f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-5e658889c567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_epocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-100-4dab614dc424>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, criterion, optimizer, numer_of_epoch, train_data_loader, validation_data_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-a4dfc67e0e0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, confusion_matrix, optimizer, criterion, input_transformation)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RS1(1)",
      "language": "python",
      "name": "rs1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "advanced_models_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}