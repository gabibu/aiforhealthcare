{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AgBH-n-LuIVH"
      },
      "outputs": [],
      "source": [
        "#baseline model UNET\n",
        "# I splitted every image to patches of 64 * 64 pixels \n",
        "#and classified every pixel in every patch as binary classification  task\n",
        "\n",
        "\n",
        "#so for the model the input is 64 * 64 pixels images.\n",
        "\n",
        "\n",
        "#model: UNET\n",
        "#loss Binary cross entropy \n",
        "#optimizer: SGD\n",
        "\n",
        "#I splitted to dataset under to dicom-images-train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "id": "8A_WGEQ2veoM",
        "outputId": "1a7f3ca2-8cf1-4a62-dd05-7303abb1bf82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d0dXiKcpuIVI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from skimage import io, transform\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from imageio import imread\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pydicom\n",
        "from abc import ABC, abstractmethod\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, RandomRotation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask2rle(img, width, height):\n",
        "    rle = []\n",
        "    lastColor = 0;\n",
        "    currentPixel = 0;\n",
        "    runStart = -1;\n",
        "    runLength = 0;\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            currentColor = img[x][y]\n",
        "            if currentColor != lastColor:\n",
        "                if currentColor == 255:\n",
        "                    runStart = currentPixel;\n",
        "                    runLength = 1;\n",
        "                else:\n",
        "                    rle.append(str(runStart));\n",
        "                    rle.append(str(runLength));\n",
        "                    runStart = -1;\n",
        "                    runLength = 0;\n",
        "                    currentPixel = 0;\n",
        "            elif runStart > -1:\n",
        "                runLength += 1\n",
        "            lastColor = currentColor;\n",
        "            currentPixel+=1;\n",
        "\n",
        "    return \" \".join(rle)\n",
        "\n",
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle.split()])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    for index, start in enumerate(starts):\n",
        "        current_position += start\n",
        "        mask[current_position:current_position+lengths[index]] = 255\n",
        "        current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height)\n"
      ],
      "metadata": {
        "id": "9WobemP_uaCJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7I1nfKTOuIVJ",
        "outputId": "6c2089ae-e8ee-4cb3-fec8-afb677ca609c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__pyTorch VERSION: 1.11.0+cu113\n",
            "use_cuda = True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use_cuda = {0}'.format(use_cuda))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DATA_FOLDER ='data'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KSClFvEdufgZ",
        "outputId": "505384e8-8317-4cf1-8ba2-42839bf5689d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/medical\"\n"
      ],
      "metadata": {
        "id": "TegtM3JzukYY",
        "outputId": "db9a8089-4116-4d3c-eefa-033d1360d119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dicom-images-train.zip\ttrain-rle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/medical/dicom-images-train.zip\" \"dicom-images-train.zip\"\n"
      ],
      "metadata": {
        "id": "HqnhMTHTumyP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/medical/train-rle.csv' \"train-rle.csv\""
      ],
      "metadata": {
        "id": "kS2rUiZkup8Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "_zhOcM3Gusxj",
        "outputId": "6490b414-3d4e-42af-ebc8-f806bd82f60b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  dicom-images-train.zip  drive  sample_data  train-rle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dicom-images-train.zip -d data"
      ],
      "metadata": {
        "id": "WrcW5B7Yu6U-",
        "outputId": "06eea54a-11e3-494c-91b2-34296e4e9981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dicom-images-train.zip\n",
            "replace data/dicom-images-train/1.2.276.0.7230010.3.1.2.8323329.4366.1517875182.502231/1.2.276.0.7230010.3.1.3.8323329.4366.1517875182.502230/1.2.276.0.7230010.3.1.4.8323329.4366.1517875182.502232.dcm? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z-Sn3O5DuIVK",
        "outputId": "6c6405d8-d62b-4cee-d5fa-3d7277d1878e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "SEED = 999\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "fixSeed(SEED)\n",
        "SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6RpGMsUeuIVK"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('archive/train-rle.csv', header=None, index_col=0)\n",
        "# train_fns = sorted(glob.glob('archive/dicom-images-train/*/*/*.dcm'))#[0:500]\n",
        "# df_full = pd.read_csv('archive/train-rle.csv', index_col='ImageId')\n",
        "\n",
        "df = pd.read_csv('train-rle.csv', header=None, index_col=0)\n",
        "train_fns = sorted(glob.glob('data/dicom-images-train/*/*/*.dcm')) #[0:500]\n",
        "len(train_fns)\n",
        "df_full = pd.read_csv('train-rle.csv', index_col='ImageId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlCbOf8YuIVK",
        "outputId": "2c4e3ab6-ed8c-4f9d-e445-eaff3c8c3d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "im_height = 1024\n",
        "im_width = 1024\n",
        "im_chan = 1\n",
        "X_s = np.zeros((len(train_fns), im_height, im_width, im_chan), dtype=np.uint8)\n",
        "Y_s = np.zeros((len(train_fns), im_height, im_width, 1), dtype=np.bool)\n",
        "sys.stdout.flush()\n",
        "for n, _id in tqdm(enumerate(train_fns), total=len(train_fns)):\n",
        "    dataset = pydicom.read_file(_id)\n",
        "    X_s[n] = np.expand_dims(dataset.pixel_array, axis=2)\n",
        "    try:\n",
        "        if '-1' in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "            Y_s[n] = np.zeros((1024, 1024, 1))\n",
        "        else:\n",
        "            if type(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']) == str:\n",
        "                Y_s[n] = np.expand_dims(rle2mask(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels'], 1024, 1024), axis=2)\n",
        "            else:\n",
        "                Y_s[n] = np.zeros((1024, 1024, 1))\n",
        "                for x in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "                    Y_s[n] =  Y_s[n] + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n",
        "    except KeyError:\n",
        "        print(f\"Key {_id.split('/')[-1][:-4]} without mask, assuming healthy patient.\")\n",
        "        Y_s[n] = np.zeros((1024, 1024, 1)) # Assume missing masks are empty masks.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UtCqL5ruIVL",
        "outputId": "ac4527b2-39a0-4cb9-e6cb-8298085b86f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 1024, 1024, 1), (500, 1024, 1024, 1))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_s.shape, Y_s.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7_p-uiouIVL",
        "outputId": "a69e7fd4-d29e-41ab-b341-bfbcd813f9e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((450, 1024, 1024, 1), (450, 1024, 1024, 1))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_size = int(Y_s.shape[0] * 0.1)\n",
        "validation_indexes = np.random.choice(range(0, Y_s.shape[0]), size=validation_size, replace=False)\n",
        "train_indexes  = [i for i in range(0, Y_s.shape[0]) if i not in validation_indexes]\n",
        "\n",
        "\n",
        "y_train = Y_s[train_indexes, :, :]\n",
        "y_validation = Y_s[validation_indexes, :, :]\n",
        "\n",
        "\n",
        "\n",
        "X_validation = X_s[validation_indexes, :, :]\n",
        "X_train = X_s[train_indexes, :, :]\n",
        "\n",
        "y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe1VQZjDuIVM",
        "outputId": "f7d87a1c-f657-4513-fc12-c97fc9d0a47a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((115200, 64, 64, 1), (115200, 64, 64, 1))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "im_height = 64\n",
        "im_width = 64\n",
        "X_train = X_train.reshape((-1, im_height, im_width, 1))\n",
        "y_train = y_train.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "\n",
        "X_validation = X_validation.reshape((-1, im_height, im_width, 1))\n",
        "y_validation = y_validation.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l87g-WRAuIVM",
        "outputId": "f1a2c652-9511-47ca-cad4-49f6b2168ad9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1539299, 470319901)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_of_positive_pixels = np.sum(y_train)\n",
        "num_of_negatuve_pixels = y_train.size - num_of_positive_pixels \n",
        "\n",
        "num_of_positive_pixels, num_of_negatuve_pixels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p_UIopmuIVN"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, images, masks, preprocesing = None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.preprocesing = preprocesing\n",
        "        \n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.masks.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        y = self.masks[index]\n",
        "        y = np.where(y == True, 1, 0)\n",
        "        \n",
        "        image  = X_train[index]\n",
        "        \n",
        "        if self.preprocesing is not None:\n",
        "            for pre_processor in self.preprocesing:\n",
        "                \n",
        "                image = pre_processor.preprocess(image)\n",
        "                \n",
        "        \n",
        "        image = image.reshape(1, image.shape[0], image.shape[1])\n",
        "        y = y.reshape(1, y.shape[0], y.shape[1])\n",
        "        \n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkDNdPTkuIVN",
        "outputId": "3f78a06e-5f80-4393-9c89-c85670f1183e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((115200, 64, 64, 1), (115200, 64, 64, 1))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCGJAk2uIVO"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekzsQ4BmuIVO"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, confusion_matrix, optimizer, criterion, input_transformation=None):\n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "\n",
        "        calc_count += labels.shape[0]\n",
        "        images = images.float().to(device)\n",
        "     \n",
        "        labels = labels.float().to(device)\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        calc_loss += loss.item()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    \n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            calc_count += labels.shape[0]\n",
        "            images = images.float().to(device)\n",
        "\n",
        "            labels = labels.float().to(device)\n",
        "            images = Variable(images)\n",
        "            labels = Variable(labels)\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            calc_loss += loss.item()\n",
        "            \n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oIlGdpEuIVP"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, criterion, optimizer, numer_of_epoch, train_data_loader, validation_data_loader):\n",
        "    \n",
        "    for epoch in range(10):\n",
        "        print(epoch)\n",
        "        train_loss = train(model, train_data_loader, None, optimizer, criterion, None)\n",
        "        val_loss = eval_model(model, validation_data_loader, criterion)\n",
        "\n",
        "        print('epoch {} train loss {} val loss {}'.format(epoch, train_loss, val_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUKbFys9uIVP"
      },
      "outputs": [],
      "source": [
        "#baseline\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight=).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "#run_experiment(model, criterion, optimizer, 100, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh3YdS0TuIVP"
      },
      "outputs": [],
      "source": [
        "#weighted loss \n",
        "\n",
        "pos_weight = num_of_positive_pixels, num_of_negatuve_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment(model, criterion, optimizer, 100, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK7yaj9MuIVQ"
      },
      "outputs": [],
      "source": [
        "class PreProcessingStep(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def preprocess(image):\n",
        "        pass\n",
        "\n",
        "    \n",
        "class PreProcessingWindow(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, preprocess_window, output_range):\n",
        "        self._preprocess_window = preprocess_window \n",
        "        self._output_range = output_range\n",
        "    \n",
        "    def preprocess(self, image):\n",
        "        windowed_image = np.interp(image, self._preprocess_window, self._output_range)\n",
        "        return windowed_image\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls-NOpWRuIVQ"
      },
      "outputs": [],
      "source": [
        "#weighted loss with image preprocessing\n",
        "\n",
        "pos_weight = num_of_positive_pixels, num_of_negatuve_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "preprocesing = [PreProcessingWindow((-300, 700), (0., 1.))]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment(model, criterion, optimizer, 100, train_data_loader, validation_data_loader)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RS1(1)",
      "language": "python",
      "name": "rs1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "advanced_models_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}