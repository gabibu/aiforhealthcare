{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AgBH-n-LuIVH"
      },
      "outputs": [],
      "source": [
        "#baseline model UNET\n",
        "# I splitted every image to patches of 64 * 64 pixels \n",
        "#and classified every pixel in every patch as binary classification  task\n",
        "\n",
        "\n",
        "#so for the model the input is 64 * 64 pixels images.\n",
        "\n",
        "\n",
        "#model: UNET\n",
        "#loss Binary cross entropy \n",
        "#optimizer: SGD\n",
        "\n",
        "#I splitted to dataset under to dicom-images-train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_WGEQ2veoM",
        "outputId": "8500bcf4-130c-4032-b21b-632a84b03979"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d0dXiKcpuIVI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, WeightedRandomSampler\n",
        "from skimage import io, transform\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from imageio import imread\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import cm\n",
        "\n",
        "import pydicom\n",
        "from abc import ABC, abstractmethod\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, RandomRotation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask2rle(img, width, height):\n",
        "    rle = []\n",
        "    lastColor = 0;\n",
        "    currentPixel = 0;\n",
        "    runStart = -1;\n",
        "    runLength = 0;\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            currentColor = img[x][y]\n",
        "            if currentColor != lastColor:\n",
        "                if currentColor == 255:\n",
        "                    runStart = currentPixel;\n",
        "                    runLength = 1;\n",
        "                else:\n",
        "                    rle.append(str(runStart));\n",
        "                    rle.append(str(runLength));\n",
        "                    runStart = -1;\n",
        "                    runLength = 0;\n",
        "                    currentPixel = 0;\n",
        "            elif runStart > -1:\n",
        "                runLength += 1\n",
        "            lastColor = currentColor;\n",
        "            currentPixel+=1;\n",
        "\n",
        "    return \" \".join(rle)\n",
        "\n",
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle.split()])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    for index, start in enumerate(starts):\n",
        "        current_position += start\n",
        "        mask[current_position:current_position+lengths[index]] = 255\n",
        "        current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height)\n"
      ],
      "metadata": {
        "id": "9WobemP_uaCJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7I1nfKTOuIVJ",
        "outputId": "85d1e610-1221-4486-f3cd-b0843f882011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__pyTorch VERSION: 1.11.0+cu113\n",
            "use_cuda = True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use_cuda = {0}'.format(use_cuda))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DATA_FOLDER ='data'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSClFvEdufgZ",
        "outputId": "2eb7ef50-2eda-467c-d42a-e17246ae9f9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/medical\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TegtM3JzukYY",
        "outputId": "f27014e4-add8-46b8-9f8e-0bb4eb3fc13d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aug.weights\t\tsimple_preprocessing.weights  weighted_2.weights\n",
            "dicom-images-train.zip\ttrain-rle.csv\t\t      weighted_sampling.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/medical/dicom-images-train.zip\" \"dicom-images-train.zip\"\n"
      ],
      "metadata": {
        "id": "HqnhMTHTumyP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/medical/train-rle.csv' \"train-rle.csv\""
      ],
      "metadata": {
        "id": "kS2rUiZkup8Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zhOcM3Gusxj",
        "outputId": "cfc7649b-347d-4617-d287-0ce2d638802f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dicom-images-train.zip\tdrive  sample_data  train-rle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  -q dicom-images-train.zip -d data "
      ],
      "metadata": {
        "id": "WrcW5B7Yu6U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Sn3O5DuIVK",
        "outputId": "76ccb978-cd0e-4633-e5a2-67e0c139078d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "SEED = 999\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "fixSeed(SEED)\n",
        "SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "6RpGMsUeuIVK",
        "outputId": "386afdfa-55f3-4edb-e7a8-bb44cf6d9563"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                        EncodedPixels\n",
              "ImageId                                                                                              \n",
              "1.2.276.0.7230010.3.1.4.8323329.14099.151787524...   745018 1 1024 3 1021 5 1019 8 1016 10 1015 12..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c429c9c9-74c7-4c9c-a113-08519cf4bf86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EncodedPixels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageId</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.2.276.0.7230010.3.1.4.8323329.14099.1517875249.625073</th>\n",
              "      <td>745018 1 1024 3 1021 5 1019 8 1016 10 1015 12...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c429c9c9-74c7-4c9c-a113-08519cf4bf86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c429c9c9-74c7-4c9c-a113-08519cf4bf86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c429c9c9-74c7-4c9c-a113-08519cf4bf86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# df = pd.read_csv('archive/train-rle.csv', header=None, index_col=0)\n",
        "# train_fns = sorted(glob.glob('archive/dicom-images-train/*/*/*.dcm'))#[0:500]\n",
        "# df_full = pd.read_csv('archive/train-rle.csv', index_col='ImageId')\n",
        "\n",
        "df = pd.read_csv('train-rle.csv', header=None, index_col=0)\n",
        "train_fns = sorted(glob.glob('data/dicom-images-train/*/*/*.dcm'))\n",
        "len(train_fns)\n",
        "df_full = pd.read_csv('train-rle.csv', index_col='ImageId')\n",
        "df_full.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = df_full.reset_index().drop_duplicates(['ImageId'])\n",
        "\n",
        "negative_images =  set(dataset_df[dataset_df[' EncodedPixels'].str.contains('-1')].sample(550)['ImageId'].tolist())\n",
        "\n",
        "positive_images = set(dataset_df[~dataset_df[' EncodedPixels'].str.contains('-1')].sample(550)['ImageId'].tolist())\n",
        "\n",
        "images_ids_for_trainning = set.union(negative_images, positive_images)\n",
        "len(images_ids_for_trainning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgB1KX0A0Qep",
        "outputId": "07bdb509-232d-473d-cc8d-3d61c39c8b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1100"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCbOf8YuIVK",
        "outputId": "e7f88256-053c-4723-b08e-b5ea10e04ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10712/10712 [00:15<00:00, 711.40it/s]\n"
          ]
        }
      ],
      "source": [
        "im_height = 1024\n",
        "im_width = 1024\n",
        "im_chan = 1\n",
        "valid_indexes = []\n",
        "X_s = np.zeros((len(images_ids_for_trainning), im_height, im_width, im_chan), dtype=np.uint8)\n",
        "Y_s = np.zeros((len(images_ids_for_trainning), im_height, im_width, 1), dtype=bool)\n",
        "sys.stdout.flush()\n",
        "index_to_set = 0\n",
        "\n",
        "for _id in tqdm(train_fns, total=len(train_fns)):\n",
        "\n",
        "    \n",
        "    image_id = _id.split('/')[-1].split('.dcm')[0]\n",
        "\n",
        "    \n",
        "    if image_id not in images_ids_for_trainning:\n",
        "      continue\n",
        "\n",
        "\n",
        "    dataset = pydicom.read_file(_id)\n",
        "    X_s[index_to_set] = np.expand_dims(dataset.pixel_array, axis=2)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    try:\n",
        "        if '-1' in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "            Y_s[index_to_set] = np.zeros((1024, 1024, 1))\n",
        "        else:\n",
        "            if type(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']) == str:\n",
        "                Y_s[index_to_set] = np.expand_dims(rle2mask(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels'], 1024, 1024), axis=2)\n",
        "            else:\n",
        "                Y_s[index_to_set] = np.zeros((1024, 1024, 1))\n",
        "                for x in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n",
        "                    Y_s[index_to_set] =  Y_s[index_to_set] + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n",
        "    except KeyError:\n",
        "        print(f\"Key {_id.split('/')[-1][:-4]} without mask, assuming healthy patient.\")\n",
        "        Y_s[index_to_set] = np.zeros((1024, 1024, 1)) # Assume missing masks are empty masks.\n",
        "    \n",
        "    index_to_set += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkTi_DbtBXPD",
        "outputId": "358ab15c-9875-402c-b03a-15a1bf9d51e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1100"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7_p-uiouIVL",
        "outputId": "fd96babe-1d86-4118-981e-2a2c636255fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((990, 1024, 1024, 1), (110, 1024, 1024, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "validation_size = int(Y_s.shape[0] * 0.1)\n",
        "validation_indexes = np.random.choice(range(0, Y_s.shape[0]), size=validation_size, replace=False)\n",
        "train_indexes  = [i for i in range(0, Y_s.shape[0]) if i not in validation_indexes]\n",
        "\n",
        "\n",
        "y_train = Y_s[train_indexes, :, :]\n",
        "y_validation = Y_s[validation_indexes, :, :]\n",
        "\n",
        "\n",
        "\n",
        "X_validation = X_s[validation_indexes, :, :]\n",
        "X_train = X_s[train_indexes, :, :]\n",
        "\n",
        "y_train.shape, y_validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe1VQZjDuIVM",
        "outputId": "fb64bb96-2596-4c1a-9593-f29882998629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((253440, 64, 64, 1), (253440, 64, 64, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "im_height = 64\n",
        "im_width = 64\n",
        "X_train = X_train.reshape((-1, im_height, im_width, 1))\n",
        "y_train = y_train.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "\n",
        "X_validation = X_validation.reshape((-1, im_height, im_width, 1))\n",
        "y_validation = y_validation.reshape((-1, im_height, im_width, 1))\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l87g-WRAuIVM",
        "outputId": "e0f339b7-625b-4257-a324-eca5c5f0ed96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6991840, 1031098400)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "num_of_positive_pixels = np.sum(y_train)\n",
        "num_of_negatuve_pixels = y_train.size - num_of_positive_pixels \n",
        "\n",
        "num_of_positive_pixels, num_of_negatuve_pixels "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessingStep(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def preprocess(image):\n",
        "        pass\n",
        "\n",
        "    \n",
        "class PreProcessingWindow(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, preprocess_window, output_range):\n",
        "        self._preprocess_window = preprocess_window \n",
        "        self._output_range = output_range\n",
        "    \n",
        "    def preprocess(self, image):\n",
        "        windowed_image = np.interp(image, self._preprocess_window, self._output_range)\n",
        "        return windowed_image\n",
        "\n",
        "\n",
        "class PreProcessingAugmentations(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, augmentations):\n",
        "        self.augmentations = augmentations \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "      image =Image.fromarray(image.squeeze(2))\n",
        "\n",
        "      for aug in self.augmentations:\n",
        "        image = aug(image)\n",
        "\n",
        "      return np.expand_dims(np.asarray(image), 2) \n",
        "\n",
        "class PreProcessingNormalization(PreProcessingStep):\n",
        "    \n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "\n",
        "\n",
        "      return (image  - self.mean) / self.std\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mean, std = X_train.mean(), X_train.std()\n",
        "\n",
        "preprocessing_normalization = PreProcessingNormalization(mean, std)\n"
      ],
      "metadata": {
        "id": "bpDxwg_MSr0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p_UIopmuIVN"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, images, masks, preprocesing = None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.preprocesing = preprocesing\n",
        "        \n",
        "      \n",
        "    def __len__(self):\n",
        "        return self.masks.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        y = self.masks[index]\n",
        "        y = np.where(y == True, 1, 0)\n",
        "        \n",
        "        image  = X_train[index]\n",
        "        \n",
        "        if self.preprocesing is not None:\n",
        "            for pre_processor in self.preprocesing:\n",
        "                \n",
        "                image = pre_processor.preprocess(image)\n",
        "                \n",
        "        \n",
        "        image = image.reshape(1, image.shape[0], image.shape[1])\n",
        "        y = y.reshape(1, y.shape[0], y.shape[1])\n",
        "        \n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCGJAk2uIVO"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        \n",
        "        return 1 - dice\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        \n",
        "        return Dice_BCE"
      ],
      "metadata": {
        "id": "1sSwO8K9K2lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekzsQ4BmuIVO"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion):\n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "\n",
        "        calc_count += labels.shape[0]\n",
        "        images = images.float().to(device)\n",
        "     \n",
        "        labels = labels.float().to(device)\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        calc_loss += loss.item()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    \n",
        "    calc_loss = 0.\n",
        "    calc_count = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            calc_count += labels.shape[0]\n",
        "            images = images.float().to(device)\n",
        "\n",
        "            labels = labels.float().to(device)\n",
        "            images = Variable(images)\n",
        "            labels = Variable(labels)\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            calc_loss += loss.item()\n",
        "            \n",
        "        \n",
        "    return calc_loss/calc_count\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oIlGdpEuIVP"
      },
      "outputs": [],
      "source": [
        "def run_experiment(exp_name, model, criterion, optimizer, numer_of_epoch, train_data_loader, validation_data_loader):\n",
        "    \n",
        "    min_val_loss = 100000\n",
        "    weights_path = '/content/drive/My Drive/medical/{}.weights'.format(exp_name)\n",
        "\n",
        "    for epoch in range(numer_of_epoch):\n",
        "        print(epoch)\n",
        "        train_loss = train(model, train_data_loader, optimizer, criterion)\n",
        "        val_loss = eval_model(model, validation_data_loader, criterion)\n",
        "\n",
        "        if val_loss < min_val_loss:\n",
        "          min_val_loss =  val_loss\n",
        "\n",
        "         \n",
        "          torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "        print('epoch {} train loss {} val loss {}'.format(epoch, train_loss, val_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUKbFys9uIVP"
      },
      "outputs": [],
      "source": [
        "#baseline\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss().to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('baseline', model, criterion, optimizer, 100, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh3YdS0TuIVP"
      },
      "outputs": [],
      "source": [
        "#weighted loss \n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "training_generator = DataGenerator(X_train, y_train)\n",
        "validation_generator = DataGenerator(X_validation, y_validation)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('weighted_loss', model, criterion, optimizer, num_epochs, train_data_loader, validation_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls-NOpWRuIVQ"
      },
      "outputs": [],
      "source": [
        "#weighted loss with image preprocessing\n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "preprocesing = [preprocessing_normalization]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('simple_preprocessing', model, criterion, optimizer, 100, train_data_loader, validation_data_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted loss with image preprocessing and augmentations\n",
        "\n",
        "pos_weight = num_of_negatuve_pixels/num_of_positive_pixels \n",
        "pos_weight = torch.as_tensor(pos_weight, dtype=torch.float).to(device)\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "number_of_epocs = 2\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss(pos_weight = pos_weight).to(device) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "aug_preprocessing = PreProcessingAugmentations([RandomHorizontalFlip(0.5), RandomVerticalFlip(0.5), RandomRotation(0.3)])\n",
        "\n",
        "preprocesing = [preprocessing_normalization, aug_preprocessing]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=8, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('aug', model, criterion, optimizer, number_of_epocs, train_data_loader, validation_data_loader)\n"
      ],
      "metadata": {
        "id": "YwE3VBgGEdOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train.shape, \n",
        "slices_positive_counts = np.sum(y_train, axis=(1,2,3))\n",
        "positive_slices = set(np.where(slices_positive_counts > 0)[0])\n",
        "\n",
        "total_slices = float(y_train.shape[0])\n",
        "num_of_negative_slices = y_train.shape[0] - len(positive_slices)\n",
        "num_of_positive_slices = len(positive_slices)\n",
        "\n",
        "weights = [1.0/num_of_positive_slices  if  i in positive_slices else 1.0/num_of_negative_slices for i in range(y_train.shape[0])]"
      ],
      "metadata": {
        "id": "Zh6p6qTyDX6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling positive and negative slices ~equally using WeightedRandomSampler\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.0001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "number_of_epocs = 50\n",
        "\n",
        "criterion =nn.BCEWithLogitsLoss().to(device) \n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "aug_preprocessing = PreProcessingAugmentations([RandomHorizontalFlip(0.5), RandomVerticalFlip(0.5), RandomRotation(0.3)])\n",
        "\n",
        "preprocesing = [preprocessing_normalization, aug_preprocessing]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=batch_size, num_workers=0, sampler = WeightedRandomSampler(weights, batch_size, replacement=False))\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('weighted_sampling',model, criterion, optimizer, number_of_epocs, train_data_loader, validation_data_loader)"
      ],
      "metadata": {
        "id": "4TmZdhS6DRqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling positive and negative slices ~equally using WeightedRandomSampler\n",
        "# loss = DiceBCELoss\n",
        "\n",
        "model = UNet(1, 1, False).to(device)\n",
        "lr = 0.0001\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "momentum = 0.9\n",
        "number_of_epocs = 50\n",
        "\n",
        "criterion = DiceBCELoss().to(device) \n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "aug_preprocessing = PreProcessingAugmentations([RandomHorizontalFlip(0.5), RandomVerticalFlip(0.5), RandomRotation(0.3)])\n",
        "\n",
        "preprocesing = [preprocessing_normalization, aug_preprocessing]\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, preprocesing)\n",
        "validation_generator = DataGenerator(X_validation, y_validation, preprocesing)\n",
        "\n",
        "train_data_loader = DataLoader(training_generator, batch_size=batch_size, num_workers=0, sampler = WeightedRandomSampler(weights, batch_size, replacement=False))\n",
        "validation_data_loader = DataLoader(validation_generator, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "\n",
        "run_experiment('weighted_2', model, criterion, optimizer, number_of_epocs, train_data_loader, validation_data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U3YZQRuM11L",
        "outputId": "359e3a43-5ac4-4bed-b985-4be1f55dfd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py:146: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
            "  return default_collate([torch.as_tensor(b) for b in batch])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train loss 0.11025986075401306 val loss 0.10859111020002853\n",
            "1\n",
            "epoch 1 train loss 0.11124366521835327 val loss 0.10854445393197239\n",
            "2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RS1(1)",
      "language": "python",
      "name": "rs1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "advanced_models_colab.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}